{"name":"Test-repo","tagline":"For testing purposes.","body":"# Orchestra\r\n\r\n## Warnings and Disclaimers\r\n\r\n**WARNING** **WARNING** **WARNING** **WARNING**\r\n\r\nMoving machines can cause damage to personal property, personal injury or death. It is the responsibility of the reader of this document to take adequate safety precautions when operating, designing, or building any machinery, or when offering machinery to others for use.\r\n\r\nNone of the software, documents, firmware, schematics, drawings or other materials accompanying this document have been checked for errors, omissions or mistakes of any kind. Use it at your own risk.\r\n\r\nYour use of the software, documents, firmware, schematics, drawings or other materials accompanying this document is governed by the [License Agreement](../LICENSE.txt). Read it before proceeding.\r\n\r\n## Contents\r\n\r\n* [History, Concept and Intent](#history-concept-and-intent)\r\n* [What is the Orchestra system](#what-is-the-orchestra-system)\r\n* [This Document](#this-document)\r\n* [Architecture](#architecture)\r\n* [Getting Started](#getting-started)\r\n* [Reference](#reference)\r\n\t* [Requirements and Dependencies](#requirements-and-dependencies)\r\n\t* [Configuration](#configuration)\r\n\r\nCopyright Notice:\r\nCopyright 2013 Google Inc. All other copyrights and trademarks are property of their respective owners.\r\n\r\n----------------------------------------------------------------\r\n\r\n## History, Concept and Intent\r\n\r\nThe Universal Orchestra was originally conceived as one of five experiments devised for an interactive exhibit at the [Science Museum, London](http://www.sciencemuseum.org.uk/) called [Chrome Web Lab](http://www.chromeweblab.com/). These five experiments were created to demonstrate how modern Web technologies can be used together for purposes that go beyond “normal” webapps.  \r\n\r\nChrome Web Lab was made by Google Creative Labs, Tellart, B-Reel, Universal Design Studio, Fraser Randall, MAP, Karsten Schmidt, Weir+Wong and Bibliotheque. These five experiments can be accessed in person by visiting the Science Museum in London or over the web at [chromeweblab.com](http://www.chromeweblab.com/). Chrome Web Lab will run from July 2012 until July 2013.\r\n\r\nOrchestra is designed for people across the world to make music collaboratively. Online visitors interact with Orchestra by selecting notes on a web-based time scale for a particular instrument. As the sequencer loops, it reads the selected note and then triggers the appropriate actuator for the instrument at the Science Museum in London.\r\n\r\n![Screenshot showing video feed of an Orchestra instrument live from the Science Museum, London](docs/images/history1.png \"Orchestra History\")   ![Screenshot showing the Web user interface for the Orchestra](docs/images/history2.png \"Orchestra History\")\r\n\r\nThe intent of this open source project is to release the base software system for a collaborative, real time loop sequencer to control physical hardware.\r\n\r\n----------------------------------------------------------------\r\n\r\n## What is the Orchestra system?\r\n\r\nThe Web Lab Orchestra allows multiple users to control physical hardware,\r\ncollaboratively and in real time, using a browser-based loop sequencer interface.\r\n\r\nThis repository contains all the [software](sw) you need to run your own version,\r\nplus some instructions to help you get started with your own [hardware](hw).\r\n\r\n## This Document\r\n\r\nIn this document we will look briefly at how the Orchestra system is organized, followed by step-by-step instructions for getting started.\r\n\r\n## Architecture\r\n\r\nThe system is organized into modules, each of which has a specific purpose. The modules communicate with each other via web sockets and MIDI. When the optional live video+audio components are used then either WebRTC or Adobe's proprietary RTMP streaming protocol are used (depending on your setup).\r\n\r\n![Diagram showing how Orchestra modules communicate with each other](docs/images/orchestra_arch_main.png \"Orchestra Modules\")\r\n\r\n- The **UI** is an HTML5 frontend (which is served via and communicates with\r\n  the realtime server)\r\n- The **realtime server** is a Node.js backend which accepts WebSockets\r\n  connections from the other components, keeping them all in sync\r\n- The **hub** is a combination of Python-based daemon and Max patch which commands MIDI hardware based on state data from the realtime server\r\n\r\nOptionally, the system can be configured to stream live audio and video from one computer back to the UI. This is intended to allow someone to play an instrument in one location from a computer in another location and be able to hear the results.\r\n\r\nThe live audio and video streaming components can be configured to use the WebRTC standard, or use Flash video. The WebRTC setup is preferred because it involves running fewer pieces of software. However, WebRTC is in active development so its performance today may still have much room for improvement.\r\n\r\n![Diagram showing how WebRC and Flash video can be used with the Orchestra](docs/images/orchestra_arch_streamer.png)\r\n\r\n\r\n## Getting Started\r\n\r\n### Assumptions\r\n\r\n> These instructions assume:\r\n* ...you will be running all software on a **single** computer which is running either Linux or Mac OS X. We will call this \"the computer\" from now on. The components can also be configured to each run on different hosts\r\n(as they do in [the Chrome Web Lab](http://www.chromeweblab.com/)).\r\nSee the [Configuration](#configuration) section below.\r\n* ...you have some development knowledge (ideally in JavaScript, Python and Max) and are comfortable typing commands into a shell or the Mac OS X Terminal.\r\n* ...you have already cloned, forked or downloaded the [project repository](../../) onto the computer. If you haven't, just download it [here](../../archive/master.zip). You can also click the \"Code\" link at the top of this page and look at all of the cloning and forking options available.\r\n\r\n### Running\r\n\r\nThe Orchestra project contains a git submodule for the signalmaster WebRTC component. It is important to understand that there are some potential issues when using git submodules. Please read this [note from the git documentation](http://git-scm.com/book/en/Git-Tools-Submodules#Issues-with-Submodules) before proceeding. Essentially, though, if you don't use branches within the submodule then you should be safe.\r\n\r\nBefore start any of the Orchestra components, please update the repository's submodules via the GitHub app, or by running this shell command from the repository [root folder](../):\r\n\r\n`git submodule update --init`\r\n\r\n\r\n#### Realtime server\r\n\r\n1. Download and install the latest version of Node.js from [this page](http://nodejs.org/).\r\n2. Start a shell or Terminal and change to the [**sw/realtimeserver** folder](sw/realtimeserver).\r\n3. Run\r\n    ```\r\n    node server.js\r\n    ```\r\n\r\nThe server's websocket connection will now be available at `localhost:8080`.\r\n\r\nYou may run it locally or on a 3rd party host such as Joyent.\r\n\r\n#### Hub\r\n\r\n1. You will need Cycling 74's [Max](http://cycling74.com/products/max). If you do not have Max already, you can download a [free trial of the full product](http://cycling74.com/downloads/). There is likely a time limit on the free trial. You can also get their free [runtime](http://cycling74.com/downloads/runtime/) which will let you use the Hub, but won't let you edit it.\r\n2. Read our guide on [how we configured Max](MAX.md)\r\n3. Open [sequencer.maxpat](Orchestra/sw/hub/max/sequencer.maxpat), from the [sw/hub/max](Orchestra/sw/hub/max) folder, in Max, the main Max patch. **Note**: Max will allow you to open and run more than one copy of the same file. Please Make sure only one copy of the sequencer patch\r\nis running at a time!\r\n4. Double-click the `[noteout]` box and make sure your MIDI interface is selected.\r\n(If you have a full version of Max, you can pre-configure this.)\r\n\r\nYou may stop and re-start the sequencer with the space bar.\r\n\r\nThe Python portion of the Hub is started separately:\r\n\r\n1. Start a shell or Terminal and change to the [**sw/hub** folder](sw/hub).\r\n2. Then, to start the Python daemon, run:\r\n      ```\r\n      ./script/start.sh\r\n      ```\r\n    \r\n    To stop it, run:\r\n      ```\r\n      ./script/stop.sh\r\n      ```\r\n\r\nYou can tail the server logs in `log/twistd.log`.\r\n\r\n#### Streaming Audio and Video\r\n\r\nStreaming live audio and video of the orchestra back to the UI is optional. If you want to use this component you first need to chose if you will use WebRTC or Flash video. See the [Architecture](#architecture) and [Streamer Requirements and Dependencies](#streamer) sections for a description of the options.\r\n\r\n##### WebRTC\r\n\r\n1. The [signalmaster](https://github.com/andyet/signalmaster) WebRTC signalling application is included as a submodule in this repository so that you should not need to manually download the source. If for some reason you need it, the **signalmaster** repository can be cloned or downloaded from GitHub at [https://github.com/andyet/signalmaster](https://github.com/andyet/signalmaster).\r\n2. Start a shell or Terminal and change to the the [main repository folder](../).\r\n3. Run this command to make sure that your copy of **signalmaster** is up-to-date: `git submodule update`. **NOTE**: If please be sure to run `git submodule update --init` if you have just cloned the repository.\r\n4. Change to the the [Orchestra/sw/streamer/signalmaster](sw/streamer/signalmaster) folder.\r\n5. Install its dependencies by running `npm install`.\r\n6. Make sure the realtime server is running. If it is not, see the [realtime server section above](#getting-started).\r\n7. Start **signalmaster** by running `node server.js` from the [Orchestra/sw/streamer/signalmaster](sw/streamer/signalmaster) folder.\r\n8. Then open `http://localhost:8081/webrtc-streamer` in Chrome. You will be prompted to allow camera access. A pull-down menu allows you to select which instrument to stream to.\r\n\r\nMultiple streamers may access the same camera on the same computer, if you would like all instruments to display the same stream.\r\n\r\nYou can now skip ahead to the [UI section](#ui).\r\n\r\n##### Flash video\r\n\r\nIf you have already set up WebRTC video streaming you can skip ahead to the [UI section](#ui).\r\n\r\nThese instructions assume you will be using the open source Red5 Flash-compatible media server. Other systems, such as Adobe's Flash Media Server, should also work, though we do not have instructions for them here.\r\n\r\n1. Download and install the version 1.0 of the Red5 server from [this page](http://www.red5.org/red5-server/)\r\n2. Start a shell or Terminal. `cd` to the Red5 directory and run\r\n    ```\r\n    ./red5.sh\r\n    ```\r\n\r\n2. We now must set up the default webapp. Once the server has booted up, go to\r\n  [http://localhost:5080/installer](http://localhost:5080/installer)\r\n  in your browser\r\n3. Install `oflaDemo` (checking the File Name column to make sure it's\r\n  the right one). After doing this once, the webapp will remain installed in this Red5 deployment.\r\n\r\n4. Download and install Adobe's Flash Media Live Encoder (FMLE) from [this page](http://www.adobe.com/products/flash-media-encoder.html).\r\n5. Start FMLE.\r\n6. Load the streaming profile included with the Orchestra system by opening [`stream-to-local-red5.xml`](sw/streamer/stream-to-local-red5.xml) in FMLE.\r\n7. click Start to begin streaming\r\n\r\nTo set up multiple streamers locally, the simplest would be to have each\r\nstreamer on a separate machine, with its own copy of FMLE and Red5.\r\nYou may of course run all of the streams (from multiple instances of FMLE)\r\nthrough a single Red5 or Flash Media Server if you wish; however, the\r\nconfiguration for that scenario is not provided here.\r\n\r\nOr you can simply use one video stream for all clients by duplicating\r\nthe RTMP URLs in `VideoStreamPaths.js` (see **Configuration** above).\r\n\r\n#### UI\r\n\r\n1. Make sure the realtime server is running. If it is not, see the [realtime server section above](#getting-started).\r\n2. Open `http://localhost:8081/ui` in Chrome.\r\n\r\nThere you will see menus to select which instrument you'd like to play,\r\nand what type of video stream (if any) you'd like to sync to.\r\n\r\n**Note**: If you don't use any video stream, it is highly recommended that you\r\nrun the **UI** Chrome clients on the same local network as the **hub**, and slave \r\nthem to the **hub** machine via NTP. (This is easily done on Mac OS by entering\r\nthe **hub**'s IP in the `Set date and time automatically` field in\r\n`System Preferences` > `Date & Time` on the Chrome client machine.) This ensures\r\nthat their system clocks are in sync, so that the timing of the sequencer on\r\nthe screen matches the timing of the **hub** playback.\r\n\r\n##### Importing MIDI files\r\n\r\nYou can import a MIDI clip by dropping a MIDI file on the file input.\r\n\r\nThe MIDI file _must_ be in this format:\r\n\r\n- Two bars of 4/4 (i.e. 8 quarter notes)\r\n- Quantized to eighth notes\r\n- Counting up from middle C (a.k.a. MIDI pitch 60)\r\n\r\n### Sound\r\n\r\nOnce you've got all the software components up and running, you can begin to\r\nwork on the sound of the music that's played back.\r\n\r\n#### Soft synth\r\n\r\nIf you just want to hear something play back without setting up any hardware,\r\nyou can download a software synthesizer (or \"soft synth\"). There are many\r\noptions, some of which are standalone, some of which require a plugin (e.g. VST)\r\nhost. One good standalone synth for Mac + Windows is\r\n[circle](http://www.futureaudioworkshop.com/circle/).\r\n\r\nBy default, the MIDI pitches output by the Hub are all sequential, i.e., the\r\nchromatic scale. (See [playback.js](sw/hub/max/playback.js).). You can map them\r\nto a different tuning in [pitch-mappings.txt](sw/hub/max/pitch-mappings.txt),\r\nwhich is a list of key-value pairs in Max's `coll` file format. So to map MIDI\r\npitch 61 to pitch 62, you would do:\r\n\r\n    61, 62;\r\n\r\nIf a mapping is not present for a specific pitch, the original pitch is output.\r\n\r\n### Physical actuators\r\n\r\nIf you'd like to experiment with building your own actuated instruments, please\r\nsee the [hardware documentation](hw).\r\n\r\n\r\n----------------------------------------------------------------\r\n\r\n## Reference\r\n\r\n### Requirements and Dependencies\r\n\r\n#### Realtime server\r\n\r\nThe server is built on [Node.js](http://nodejs.org/), with no package dependencies.\r\n\r\n#### Hub\r\n\r\nThe Hub consists of two subcomponents—a **Python WebSockets client** and a\r\n**Max patch**, which communicate with one another locally via UDP (OpenSoundControl).\r\nThe motivation for this duality is to combine the benefits of MaxMSP's musically-precise\r\ntiming with Python's robust networking functionality.\r\n\r\n##### Python\r\n\r\nThe Python client has been tested with [Python 2.7](http://www.python.org/download/releases/2.7/),\r\nusing [twistd](http://twistedmatrix.com/documents/current/core/howto/basics.html) to run as a daemon.\r\n\r\nTo install the required Python packages, first [install pip](http://www.pip-installer.org/en/latest/installing.html)\r\n(using `get-pip.py` or otherwise). Once pip is installed:\r\n\r\n1. Open the Terminal or shell and change to the [Orchestra/sw/hub](sw/hub) folder\r\n2. Run this command from that folder:\r\n\r\n    `sudo pip install -r requirements.txt`\r\n\r\nYou may instaed install the individual Python packages listed in [Orchestra/sw/hub/requirements.txt](sw/hub/requirements.txt).\r\n\r\n##### Max\r\n\r\nYou may use either the full or free Runtime version of\r\n[Max 5.1.9](http://cycling74.com/downloads/older/).\r\n\r\n#### Streamer\r\n\r\nYou may select one of two video streaming platforms: Flash video or WebRTC.\r\n\r\nFlash video is used in [the Chrome Web Lab](http://www.chromeweblab.com/),\r\nas it can be synced to sequencer data from the realtime server in the UI.\r\n\r\n**Caveat emptor**: WebRTC video has no mechanism to sync the audio/video stream\r\nwith the note sequencer (as Flash video does), so it will have a noticeable\r\ndelay with regards to the sequencer, especially on the open internet.\r\n\r\n##### WebRTC\r\n\r\nThe browser JavaScript for broadcasting and viewing WebRTC streams,\r\n[a custom fork of SimpleWebRTC](https://github.com/Tellart/SimpleWebRTC),\r\nis included in this repository, so you don't need to install anything.\r\n\r\nSimpleWebRTC has a companion Node.js backend (for signaling in between browsers)\r\ncalled [signalmaster](https://github.com/andyet/signalmaster). It is recommended\r\nthat you clone it from GitHub and install its dependencies using\r\n[`npm`](https://npmjs.org/).\r\n\r\n##### Flash video\r\n\r\nIf you won't be using WebRTC you can also use Flash video to stream. For this, you'll need an encoder to capture the video from the camera and a Flash Media Server to relay the stream to the end client.\r\n\r\nFor Flash video, the Orchestra works with Adobe's \r\n[Flash Media Live Encoder 3.2](http://www.adobe.com/products/flash-media-encoder.html).\r\n\r\nThe [Red5 Media Server 1.0](http://www.red5.org/) is an open source alternative\r\nto Adobe's commercial Flash Media Server product.\r\n\r\nYou may of course use a paid streaming host in the cloud, but do make sure the latency is\r\nas low as possible (ideally not more than a few seconds).\r\n\r\n\r\n#### UI\r\n\r\nA modern HTML5-compatible browser, such as [Chrome](http://www.google.com/chrome) or [Webkit](http://www.webkit.org/)\r\nis required to use the UI. The UI prioritizes experimentation and ease of disassembly, and does\r\nnot contain any complex code to make it work in older browsers.\r\n\r\n\r\n### Configuration\r\n\r\n#### Deploying components on different hosts\r\n\r\nUnless you are running all components of the Orchestra on a single host, you\r\nwill need to configure each to point to certain others.\r\n\r\nIf you deploy the **realtime server** on a separate host:\r\n\r\n- In the **hub**, update the `host` in [app.yml](sw/hub/config/app.yml)\r\n- In the **UI**, update the `REALTIME_SERVER_HOST` in\r\n  [OrchestraConfiguration.js](sw/realtimeserver/static/ui/js/weblab/orchestra/OrchestraConfiguration.js)\r\n\r\nIf you are using Flash video, and move the Red5 or Flash Media Server to another host:\r\n\r\n- In the streamers, update the Primary Streaming Server in each instance of FMLE\r\n- In the **UI**, update `LOW_LATENCY_URLS` (and `LOW_LATENCY_IDS` as needed) in\r\n  [OrchestraConfiguration.js](sw/realtimeserver/static/ui/js/weblab/orchestra/OrchestraConfiguration.js)\r\n  (There should be one stream per instrument)\r\n\r\nIf you are using WebRTC, and move the Node.js signaling server to another host:\r\n\r\n- In the **UI**, update `WEBRTC_SIGNALING_URL` in\r\n  [OrchestraConfiguration.js](sw/realtimeserver/static/ui/js/weblab/orchestra/OrchestraConfiguration.js)\r\n  (affects **streamers**, too)\r\n\r\n#### Modifying the default number of instruments or pitches\r\n\r\n- In the **realtime server**, update [configuration.js](sw/realtimeserver/weblab/configuration.js)\r\n- In the **UI**, update\r\n  [OrchestraConfiguration.js](sw/realtimeserver/static/ui/js/weblab/orchestra/OrchestraConfiguration.js)\r\n  (and search for `NUMBER_OF_PITCHES` in [orchestra.css](sw/realtimeserver/static/ui/css/orchestra.css))\r\n- In the **hub** update `PITCHES_PER_INSTRUMENT` in [playback.js](sw/hub/max/playback.js)\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}